{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "- Read data\n",
    "- Preprocess\n",
    "- One-hot encoding\n",
    "- TF-IDF\n",
    "- n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('translated_data.csv')\n",
    "X_trans = df.to_numpy()\n",
    "X_trans = X_trans.flatten()\n",
    "\n",
    "# Read target data\n",
    "import json_lines\n",
    "yz = []\n",
    "with open('data.txt', 'rb') as f:\n",
    "    for item in json_lines.reader(f):\n",
    "        yz.append([item['voted_up'], item['early_access']])\n",
    "        \n",
    "X_train, X_test, yz_train, yz_test = train_test_split(X_trans, yz, test_size=0.3)\n",
    "\n",
    "yz_train_split = np.hsplit(np.array(yz_train), 2)\n",
    "y_train = yz_train_split[0]\n",
    "z_train = yz_train_split[1]\n",
    "\n",
    "yz_test_split = np.hsplit(np.array(yz_test), 2)\n",
    "y_test = yz_test_split[0]\n",
    "z_test = yz_test_split[1]\n",
    "\n",
    "np.save('./features/y_train', y_train)\n",
    "np.save('./features/y_test', y_test)\n",
    "\n",
    "np.save('./features/z_train', z_train)\n",
    "np.save('./features/z_test', z_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "\n",
    "def preprocess(text_data):\n",
    "    # custom punctuation filter does not include ':', '(' and ')' for emojis\n",
    "    PUNCTUATION = '!\"#$%&\\'*+,-./;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "    def remove_punc(s):\n",
    "        return \"\".join([char for char in s if char not in PUNCTUATION])\n",
    "\n",
    "    def remove_stopwords(tokens):\n",
    "        stop_words = stopwords.words('english')\n",
    "        stop_words.remove('very')\n",
    "        stop_words.remove('not')\n",
    "        return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    def stem(tokens):\n",
    "        porter = PorterStemmer()\n",
    "        return [porter.stem(word) for word in tokens]\n",
    "    \n",
    "    def encode_emojis(s):\n",
    "        s = re.sub(r'â™¥+', 'profanity', s)\n",
    "        s = s.replace(':)', 'smiley')\n",
    "        s = s.replace(':(', 'frowney')\n",
    "        s = s.replace('<3', 'heart')\n",
    "        return s\n",
    "\n",
    "    # to lowercase\n",
    "    low = list(map(str.lower, text_data))\n",
    "    emojis = list(map(encode_emojis, low))\n",
    "    punc = list(map(remove_punc, emojis))\n",
    "    tok = list(map(word_tokenize, punc))\n",
    "    stop = list(map(remove_stopwords, tok))\n",
    "    stemmed = list(map(stem, stop))\n",
    "\n",
    "    # Represent X_stemmed as a single list of documents as strings with space separated tokens\n",
    "    stemmed_flat = [' '.join(e for e in item) for item in stemmed]\n",
    "    return stemmed_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.sparse as sp\n",
    "\n",
    "X_train_prep = preprocess(X_train)\n",
    "vectorizer = CountVectorizer(max_features=3500)\n",
    "X_train_count = vectorizer.fit_transform(X_train_prep)\n",
    "sp.save_npz('./features/train_count.npz', X_train_count)\n",
    "\n",
    "X_test_prep = preprocess(X_test)\n",
    "X_test_count = vectorizer.transform(X_test_prep)\n",
    "sp.save_npz('./features/test_count.npz', X_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse as sp\n",
    "\n",
    "X_train_prep = preprocess(X_train)\n",
    "tfidf = TfidfVectorizer(max_features=3500)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_prep)\n",
    "sp.save_npz('./features/train_tfidf.npz', X_train_tfidf)\n",
    "\n",
    "X_test_prep = preprocess(X_test)\n",
    "X_test_tfidf = tfidf.transform(X_test_prep)\n",
    "sp.save_npz('./features/test_tfidf.npz', X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF with bigrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train_prep = preprocess(X_train)\n",
    "tfidf_bigram = TfidfVectorizer(max_features=3500, ngram_range=(1,2))\n",
    "X_train_tfidf_bigram = tfidf_bigram.fit_transform(X_train_prep)\n",
    "sp.save_npz('./features/train_tfidf_bigram.npz', X_train_tfidf_bigram)\n",
    "\n",
    "X_test_prep = preprocess(X_test)\n",
    "X_test_tfidf_bigram = tfidf_bigram.transform(X_test_prep)\n",
    "sp.save_npz('./features/test_tfidf_bigram.npz', X_test_tfidf_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=4614, size=50, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v = Word2Vec(X_stemmed, min_count=3, size=50)\n",
    "print(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def doc_to_vec(tokens):\n",
    "    vecs = []\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            vec = w2v[t]\n",
    "            vecs.append(vec)\n",
    "        except:\n",
    "            pass\n",
    "    if vecs != []:\n",
    "        return np.array(vecs).mean(axis=0)\n",
    "    else:\n",
    "        return np.zeros(50)\n",
    "\n",
    "X_w2v = [doc_to_vec(d) for d in X_stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./features/w2v_50.npy', X_w2v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
